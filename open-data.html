<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Open Data, Challenges towards implementation, and possible solutions.</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="buttondown.css">
</head>
<body>
<header>
<h3 class="date">January 2014</h3>

<h1 class="title">Open Data, Challenges towards implementation, and possible solutions.</h1>


<h2 class="author">Ian Mulvany</h2>
<p class="affilation"><em>eLife Sciences, Sage</em></p>

</header>


<p class="small"><strong>Abstract: </strong><em>Calls in favour of Open Data in research are becoming overwhelming. They are at national <span class="citation">(RCUK 2015)</span> and international levels <span class="citation">(Moedas 2015, <span class="citation">Boulton et al. (2012)</span>)</span>. I will set out a working definition of Open Data and will discuss the key challenges preventing publication of Open Data becoming standard practice. I will attempt to draw some general solutions to those challenges from field specific examples.</em></p>



<h1 id="open-data-is">Open Data is …</h1>
<p>Open Data is Findable, Accessible, Interoperable and Reusable <span class="citation">(Wilkinson et al. 2016)</span>. Making data available is key to support reproducibility, but by far the greatest benefit comes when data can be built upon, and in so doing can truly assist with the advancement of knowledge. In addition, one re-use of a data set effectively doubles the utility of that dataset.</p>
<h1 id="how-should-we-speak-of-the-challenges-for-open-data">How should we speak of the challenges for Open Data?</h1>
<p><span class="citation">(Goodman et al. 2014)</span> lay out ten rules for the care and feeding of scientific data. Were all of these rules to be adhered to by all researchers, we would have as good an Open Data ecosystem as we could wish for. Let us look at what might be preventing us from adopting the key practices from these rules! To streamline the discussion I have grouped the ten rules into three core challenges.</p>
<h1 id="core-challenge-one-competence-in-working-with-data">Core Challenge One: Competence in Working with Data</h1>
<p>This challenge is addressed by rules one, three and four:</p>
<ul>
<li>Rule 1. Love Your Data, and Help Others Love It, Too</li>
<li>Rule 3. Conduct Science with a Particular Level of Reuse in Mind<br />
</li>
<li>Rule 4. Publish Workflow as Context</li>
</ul>
<p>Data that is well described and well documented and that follows good data format standards, is more likely to be interoperable with similar data. Such data is more useful than were it to follow these principles.</p>
<p>A number of potential issues stand in the way of making data of this quality.</p>
<ul>
<li>Basic researcher familiarity with good data practice is low, and unfit tools are used<br />
</li>
<li>Keeping track of what the researcher did at the point of data capture can be complicated, requiring later reconstruction when tagging data<br />
</li>
<li>New scientific tools coming to market sometimes create proprietary data formats as output formats<br />
</li>
<li>researchers sometimes create custom data formats for their research (Figshare hosts over 100 distinct mimetypes)</li>
<li>Some data is hyper-hetrogeneous, bringing together multi-varied data across many dimensions, in such a way that only the researcher who created that data set understands how to unpick it</li>
</ul>
<p>Most of these issues have all been successfully addressed in specific domains or communities.</p>
<p>Software Carpentry has reached over 120,000 students <span class="citation">(Wilson 2016)</span>. They conduct two day workshops instructing researchers on the basics of how to work with software. They have created a sister organisation whose aim is to do the same, but on the basics of data management.</p>
<p>Mention https://rinocloud.com/#demo in terms of how to autogenerate good data <!-- #TODO:finish this section  --></p>
<p>Use field specific standards where they exist. Most core disciplines have well described data formats, and appropriate data repositories, but even in areas that are close, a lack of harmonisation of data standards can be an issue. Initiatives like the ISA TOOLs <span class="citation">(Sansone et al. 2012)</span> initiative can help significantly with creating interoperable data standards in the life sciences.</p>
<p>With microscopy new microscopes have frequently created new data formats. To aid with instrument interoperability the microscopy community created the <a href="http://www.openmicroscopy.org/site/products/omero">OMERO</a> framework, a set of standards and software tools, that supports interoperability across over 140 different image formats.</p>
<p>For keeping track of what happens at the point of data collection, creating smart tooling is an important advance. Also digital lab notebooks have a place to play in this. Google have created a digital lab notebook for the mass market with <a href="https://makingscience.withgoogle.com/science-journal">Google Science Journal</a> and <a href="http://jupyter.org">Project Juypter</a> offers a digital notebook that supports collaboration, computation, versioning and dissemination of scientific results.</p>
<p>Another route for creating compatible data is to follow data standards bodies. The Open Annotation working group created a data format with a high degree of usage in the digital humanities, and ensured interoperability and openness of that data format through an open standardisation process that led the format becoming a W3C standard.</p>
<h1 id="core-challenge-two-appropriate-infrastructure-for-open-data">Core Challenge Two: Appropriate Infrastructure for Open Data</h1>
<p>This challenge is addressed by rules two, six, and eight:</p>
<ul>
<li>Rule 2. Share your data online with a Permanent identifier<br />
</li>
<li>Rule 6. Publish Your Code (Even the small bits)<br />
</li>
<li>Rule 8. Foster and use data repositories</li>
</ul>
<p>Data that is identifiable, has a stable home, and good curation is data that is findable and accessible. This requires the existence of appropriate infrastructure <span class="citation">(Geoffrey, Jennifer, and Cameron 2015)</span> for that data.</p>
<p>Good infrastructure does exist for data in many domains of research (e.g. high energy physics, astronomy, genomics <span class="citation">(Benson et al. 2013, <span class="citation">Berman (2000)</span>)</span>), however there are emerging domains for whom data infrastructure is becoming a critical (e.g. high throughput and resolution microscopy, conectomics <span class="citation">(Lichtman, Pfister, and Shavit 2014)</span>, computational social science). These domains share a common pattern of tool sophistication and data generation having expanded at a rate that is much faster than had been anticipated leaving them in a momentary state of data crisis.</p>
<p>It may be instructive to look at how high energy paticle physics deals with it’s data storage requirments. CERN <span class="citation">(CERN 2009)</span> outlines four levels of data preservation</p>
<ol style="list-style-type: decimal">
<li>retain only the publication that the data ended up generating<br />
</li>
<li>preserve the data in a simplified format, this might be for outreach or training purposes</li>
<li>preserve the analysis software and specification of the data format</li>
<li>preserve the full reconstruction level data, and possibly some of the original data</li>
</ol>
<p>It is understood that much of the primary data coming out of the detector will have to be discarded, and so their data preservation framework allows them to make decisions on what to keep based around the expected future uses of that kind of data.</p>
<p>It may be possible with emerging high resolution data sources to also find ways to make decisions around whether we can preserve certain artefacts that are of lower dimensionality of the original source data. For example in conectomics one begins with high resolution images of brain slices, and a full 3-D image of a brain can be on the order of petabytes of data, however the final network diagram showing the interconnection of the neuronal scaffolding of a brain will be many orders of magnitude smaller than the original images.</p>
<!--  For bological samples, hightrougput sequencing  -->
<p>Another issue to keep in mind about 70% of the lifetime cost is incurred on first write to disk, given the current rates at which the prices of long term data storage is dropping, so the question of which data sets do we need to make strategic decisions around will probably always be with us, but our view on the kind and size of what that data is will constantly be changing.</p>
<p>In terms of numbers of data sets, most researchers producing data are not producing data at large scale. The questions around how they can create good identifiers for their data, and how they can find appropriate locations to deposit their data, are equally important as for those of large data.</p>
<p>For subject specific data there usually exists a subject specific repository. The <a href="http://www.re3data.org">Registry of Research Data Repositories</a> lists over 1500 research data repositories. The journal Scientific Data also maintains a more <a href="http://www.nature.com/sdata/policies/repositories">curated list</a>.</p>
<p>For data that does not naturally find a home in one of these repositories there are also generic data repositories such as Figshare, Zenodo, DataDryad, Imeji and Github. Each of these will allow a researcher to post a public version of their data with an appropriate identifier. The main challenge here is in increasing awareness amongst researchers about the availability of tools like these.</p>
<p>There has been a growing interest in using peer to peer like systems for building data sharing infrastructures. Though not aimed at creating public repositories, tools like <a href="http://dat-data.com">dat data</a> and <a href="https://github.com/attic-labs/noms">noms</a> have the potential to significantly improve how researchers share and collaborate on data sets with each other.</p>
<p>There are two other infrastructure challenges that we might consider around data; privacy and ownership.</p>
<p>For any</p>
<h1 id="core-challenge-three-creating-a-supporting-culture-for-openness">Core Challenge Three: Creating a supporting culture for openness</h1>
<p>This is addressed by rules five, seven, nine and ten, and can be summarised by asking how we can ensure that the correct incentives are in place to support the sharing of open data.</p>
<ul>
<li>Rule 5. Link Your Data to Your Publications as Often as Possible<br />
</li>
<li>Rule 7. State How You Want to Get Credit<br />
</li>
<li>Rule 9. Reward Colleagues Who Share Their Data Properly<br />
</li>
<li>Rule 10. Be a Booster for Data Science</li>
</ul>
<h2 id="some-solutions">Some Solutions</h2>
<ul>
<li>encourage high profile journals to advocate for open data</li>
<li>require data management plans</li>
<li>celebrate those who share well, create social pressure for data sharing<br />
</li>
<li>make data sharing mandatory PLOS March 2014.</li>
</ul>
<h1 id="summary-of-solutions">Summary of solutions</h1>
<hr />
<div id="refs" class="references">
<div id="ref-Benson2013">
<p>Benson, D. A., M. Cavanaugh, K. Clark, I. Karsch-Mizrachi, D. J. Lipman, J. Ostell, and E. W. Sayers. 2013. “GenBank.” <em>Nucleic Acids Research</em> 41 (D1): D36–D42. doi:<a href="https://doi.org/10.1093/nar/gks1195">10.1093/nar/gks1195</a>.</p>
</div>
<div id="ref-Berman2000">
<p>Berman, H. M. 2000. “The Protein Data Bank.” <em>Nucleic Acids Research</em> 28 (1): 235–42. doi:<a href="https://doi.org/10.1093/nar/28.1.235">10.1093/nar/28.1.235</a>.</p>
</div>
<div id="ref-RSOpen">
<p>Boulton, Geoffrey, Philip Campbell, Brian Collins, Peter Elias, Wendy Hall, Laurie Graeme, Onora O’Neill, et al. 2012. <em>Science as an open enterprise</em>. June. <a href="http://royalsociety.org/uploadedFiles/Royal{\_}Society{\_}Content/policy/projects/sape/2012-06-20-SAOE.pdf" class="uri">http://royalsociety.org/uploadedFiles/Royal{\_}Society{\_}Content/policy/projects/sape/2012-06-20-SAOE.pdf</a>.</p>
</div>
<div id="ref-CERN-DATA">
<p>CERN. 2009. “Data Preservation in High-Energy Physics,” no. May: 1–18.</p>
</div>
<div id="ref-Geoffrey2015">
<p>Geoffrey, Bilder, Lin Jennifer, and Neylon Cameron. 2015. “Principles for Open Scholarly Infrastructures-v1.”</p>
</div>
<div id="ref-Goodman2014">
<p>Goodman, Alyssa, Alberto Pepe, Alexander W. Blocker, Christine L. Borgman, Kyle Cranmer, Merce Crosas, Rosanne Di Stefano, et al. 2014. “Ten Simple Rules for the Care and Feeding of Scientific Data.” Edited by Philip E. Bourne. <em>PLoS Computational Biology</em> 10 (4). Public Library of Science: e1003542. doi:<a href="https://doi.org/10.1371/journal.pcbi.1003542">10.1371/journal.pcbi.1003542</a>.</p>
</div>
<div id="ref-Lichtman2014">
<p>Lichtman, Jeff W, Hanspeter Pfister, and Nir Shavit. 2014. “The big data challenges of connectomics.” <em>Nature Neuroscience</em> 17 (11). NIH Public Access: 1448–54. doi:<a href="https://doi.org/10.1038/nn.3837">10.1038/nn.3837</a>.</p>
</div>
<div id="ref-Moedas2015">
<p>Moedas, Carlos. 2015. <em>Open Innovation, Open Science, Open to the World</em>. doi:<a href="https://doi.org/10.2777/061652">10.2777/061652</a>.</p>
</div>
<div id="ref-RCKUOpen">
<p>RCUK. 2015. “Concordat On Open Research Data - Version 10,” no. July.</p>
</div>
<div id="ref-Sansone2012">
<p>Sansone, Susanna-Assunta, Philippe Rocca-Serra, Dawn Field, Eamonn Maguire, Chris Taylor, Oliver Hofmann, Hong Fang, et al. 2012. “Toward interoperable bioscience data.” <em>Nature Genetics</em> 44 (2): 121–26. doi:<a href="https://doi.org/10.1038/ng.1054">10.1038/ng.1054</a>.</p>
</div>
<div id="ref-Wilkinson2016">
<p>Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for scientific data management and stewardship.” <em>Scientific Data</em> 3 (March). Nature Publishing Group: 160018. doi:<a href="https://doi.org/10.1038/sdata.2016.18">10.1038/sdata.2016.18</a>.</p>
</div>
<div id="ref-wilson2016">
<p>Wilson, Greg. 2016. “Software Carpentry: lessons learned.” <em>F1000Research</em> 3 (January). doi:<a href="https://doi.org/10.12688/f1000research.3-62.v2">10.12688/f1000research.3-62.v2</a>.</p>
</div>
</div>
</body>
</html>
