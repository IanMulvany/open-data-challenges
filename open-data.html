<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <title>Open Data, Challenges towards implementation, and possible solutions.</title>
  <style type="text/css">code{white-space: pre;}</style>
  <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="buttondown.css">
</head>
<body>
<header>
<h3 class="date">January 2014</h3>

<h1 class="title">Open Data, Challenges towards implementation, and possible solutions.</h1>


<h2 class="author">Ian Mulvany</h2>
<p class="affilation"><em>eLife Sciences, Sage</em></p>

</header>


<p class="small"><strong>Abstract: </strong><em>Calls in favour of Open Data in research are becoming overwhelming. They are at national <span class="citation">(RCUK 2015)</span> and international levels <span class="citation">(Moedas 2015, <span class="citation">Boulton et al. (2012)</span>, <span class="citation">(“<span class="nocase">on Open Science</span>,” n.d.)</span>)</span>. I will set out a working definition of Open Data and will discuss the key challenges preventing the publication of Open Data becoming standard practice. I will attempt to draw some general solutions to those challenges from field specific examples.</em></p>



<h1 id="open-data-is">Open Data is …</h1>
<p>Open Data is Findable, Accessible, Interoperable and Reusable <span class="citation">(M. D. Wilkinson et al. 2016)</span>. Making data available is key to support reproducibility, but by far the greatest benefit comes when data can be built upon. This truly assists with the advancement of knowledge. When data is reused there is an immediate return on the investment used for the creation of the original data.</p>
<h1 id="how-should-we-speak-of-the-challenges-for-open-data">How should we speak of the challenges for Open Data?</h1>
<p><span class="citation">(Goodman et al. 2014)</span> lay out ten rules for the care and feeding of scientific data. Were all of these rules to be adhered to by all researchers, we would have as good an Open Data ecosystem as we could wish for. Let us look at what might be preventing us from adopting these key practices. To streamline the discussion I have grouped the ten rules into three core challenges.</p>
<h1 id="core-challenge-one-competence-in-working-with-data.">Core Challenge One: Competence in Working with Data.</h1>
<p>This challenge is addressed by rules one, three and four:</p>
<ul>
<li>Rule 1. Love Your Data, and Help Others Love It, Too.<br />
</li>
<li>Rule 3. Conduct Science with a Particular Level of Reuse in Mind.<br />
</li>
<li>Rule 4. Publish Workflow as Context.</li>
</ul>
<p>Data that is well described and well documented and that follows standards appropriate for it’s discipline, is more likely to be interoperable with similar data. Such data is more useful than were it to not follow these principles.</p>
<p>A number of potential issues stand in the way of generating data with this level of quality.</p>
<ul>
<li>Basic researcher familiarity with good data practice can be low.<br />
</li>
<li>Keeping track of what the researcher did at the point of data capture can be complicated, requiring later reconstruction when tagging data.<br />
</li>
<li>New scientific tools coming to market sometimes create proprietary data formats as output formats.<br />
</li>
<li>Researchers sometimes create custom data formats for their research (Figshare hosts over 100 distinct mimetypes).<br />
</li>
<li>Some data is heterogeneous, bringing together multi-varied data across many dimensions, in such a way that only the researcher who created that data set understands how to unpick it.</li>
</ul>
<p>Most of these issues have all been successfully addressed within specific domains or communities.</p>
<p>In order to improve researcher skills with working with code, Software Carpentry has reached over 120,000 students <span class="citation">(Wilson 2016)</span>. They conduct two-day workshops instructing researchers on the basics of how to work with software. They have created a sister organisation whose aim is to do the same, but on the basics of data management - Data Carpentry. This is a ground-up effort that is being sustained by the good will of the communities within which these courses happen, and nicely demonstrates the appetite for improved skills amongst software and data intensive researchers.</p>
<p>Many fields have specific standards for data description, and these should be used where they exist. Most core disciplines have appropriate data repositories, but even between similar fields, a lack of harmonisation of data standards can be an issue. The ISA TOOLs <span class="citation">(Sansone et al. 2012)</span> initiative can help significantly with creating interoperable data standards in the life sciences, and this kind of data interoperability effort is a good example for other fields that are looking for similar interoperability.</p>
<p>New microscopes have frequently created new data formats. To aid with instrument interoperability the microscopy community created the <a href="http://www.openmicroscopy.org/site/products/omero">OMERO</a> framework, a set of standards and software tools, that supports interoperability across over 140 different image formats.</p>
<p>For keeping track of what happens at the point of data collection, creating smart tooling is an important advance. Digital lab notebooks have a place to play in this. Google have created a digital lab notebook for the mass market with <a href="https://makingscience.withgoogle.com/science-journal">Google Science Journal</a>. <a href="http://jupyter.org">Project Juypter</a> offers a digital notebook that supports collaboration, computation, versioning and dissemination of scientific results. Tools can be configured to automatically annotate data at the point of data capture. <a href="https://rinocloud.com/">Rinocloud</a> offers a service that can act as a digital hub bringing together data from multiple machines, into an auto-updated lab notebook.</p>
<p>For heterogeneous datasets capturing metadata about the workflow can be as important capturing the data itself. In the life sciences workflow tools such as Galaxy<span class="citation">(Afgan et al. 2016)</span> are being increasingly used. The journal <a href="http://gigascience.biomedcentral.com">Gigascience</a> now hosts published Galaxy workflows on a sister site <a href="http://gigagalaxy.net/workflow/list_published">GigaGalaxy</a>.</p>
<p>Another route for creating compatible data is to make use of data standards bodies. The Open Annotation working group created a data format with a high degree of usage in the digital humanities, and ensured interoperability and openness of that data format through an open standardisation process that led the format becoming a World Wide Web Consortium standard.</p>
<h2 id="recommendations-for-core-challenge-one.">Recommendations for core challenge one.</h2>
<p>There exist many tools and resources for learning good data management practice. Communities are self-organising training to equip themselves with the techniques needed for working in data intensive research. Skills learnt early in a career can form the basis for ongoing improvement and learning.</p>
<p>Recommendations for dealing with skills gaps are:</p>
<ul>
<li>Coordinate training programs to adopt best practice for data management skills.<br />
</li>
<li>In these training programs include introductions to tooling that can take the burden off of the researchers for managing their data.<br />
</li>
<li>Encourage the publication and dissemination of good standards and workflows so that researchers can learn by example.</li>
</ul>
<h1 id="core-challenge-two-appropriate-infrastructure-for-open-data.">Core Challenge Two: Appropriate Infrastructure for Open Data.</h1>
<p>This challenge is addressed by rules two, six, and eight:</p>
<ul>
<li>Rule 2. Share your data online with a Permanent identifier.<br />
</li>
<li>Rule 5. Link Your Data to Your Publications as Often as Possible.<br />
</li>
<li>Rule 6. Publish Your Code (Even the small bits).<br />
</li>
<li>Rule 8. Foster and use data repositories.</li>
</ul>
<p>Data that has an identifier, a stable home, and is well curated is data is easily findable and accessible. This requires the existence of appropriate infrastructure <span class="citation">(Geoffrey, Jennifer, and Cameron 2015)</span> for that particular kind of data, be that technical infrastructure (for hosting and issuing of identifiers) or organisation and human infrastructure (for curation, annotation and preservation of the data).</p>
<p>Some challenges that exist around the creation of good infrastructures include:</p>
<ul>
<li>Some fields are experiencing an explosion of data and do not have field-wide infrastructural support for their data.<br />
</li>
<li>Some data is being created at a scale beyond the ability of even the best infrastructures to store that data.<br />
</li>
<li>Some data does not fall neatly into a subject specific repository.</li>
<li>Researchers are equally reticent to share code as they are to share data, but code sharing has some unique challenges, such as dependency management and software quality.<br />
</li>
<li>Some data needs to be treated with extreme care owing to privacy concerns, and privacy infrastructure is in it’s infancy.<br />
</li>
<li>Until recently there has been confusion around how to give credit for data contributions within the literature.</li>
</ul>
<p>Good infrastructure does exist for data in many domains of research (e.g. high energy physics <span class="citation">(CERN 2009)</span>, astronomy, genomics <span class="citation">(Benson et al. 2013, <span class="citation">Berman (2000)</span>)</span>, however there are emerging domains for whom lack of good infrastructure is becoming a critical problem (e.g. high throughput and resolution microscopy, conectomics <span class="citation">(Lichtman, Pfister, and Shavit 2014)</span>, computational social science). These domains share a common pattern where the tools used have reached new levels of sophistication and as a result data generation from these tools has expanded at a rate that is much faster than had been anticipated within their fields. This is leaving these fields in a momentary state of data crisis. A solution to these kinds of crises is to encourage the funding of data infrastructure, however whether this should be done on a project level, institution level, national level, or even international level remains an open and unresolved question. Building on many reports on the challenges of data infrastructure <span class="citation">(Knowledge Exchange 2016)</span> recommends taking into account the full research cycle when thinking about funding infrastructures, and to look to the US where the NSF has a dedicated program for the creation of shared <a href="http://www.nsf.gov/cise/aci/cif21/CIF21Vision2012current.pdf">data-centric cyber infrastructure</a>.</p>
<p>Even for for data at large scale where good infrastructure does exist, it is often not possible to preserve all of the data. It is instructive to look at how high energy particle physics deals with it’s data storage requirements. CERN <span class="citation">(CERN 2009)</span> outlines four levels of data preservation</p>
<ol style="list-style-type: decimal">
<li>retain only the publication that the data ended up generating<br />
</li>
<li>preserve the data in a simplified format, this might be for outreach or training purposes</li>
<li>preserve the analysis software and specification of the data format</li>
<li>preserve the full reconstruction level data, and possibly some of the original data</li>
</ol>
<p>It is understood that much of the primary data coming out of the detector will have to be discarded, and so their data preservation framework allows them to make decisions on what to keep based around the expected future uses of that kind of data.</p>
<p>It may be possible with other emerging high resolution data sources to also find ways to make decisions around whether we can preserve certain artefacts that are of lower dimensionality of the original source data. For example in conectomics one begins with high resolution images of brain slices, and a full 3-D image of a brain can be on the order of petabytes of data, however the final network diagram showing the interconnection of the neuronal scaffolding of a brain will be many orders of magnitude smaller than the original images.</p>
<p>Another strategy, where large scale data is concerned, is to look to use peer to peer systems for data sharing. The tool <a href="http://dat-data.com">dat data</a> uses a bit-torrent like protocol to allow the creation of a pool of nodes for sharing data. It has been successfully used to overcome network bottlenecks in the sharing of genomic data across Sudan.</p>
<p>About 70% of the lifetime cost is incurred on first write to disk, given the current rates at which the prices of long term data storage is dropping (in 1980 1GB of data storage cost $193,000, that has dropped to $0.03 in 2015 <span class="citation">(komorowski 2015)</span>), so the question of which data sets do we need to make strategic decisions around will probably always be with us, but our view on the kind and size of what that data is will constantly be changing.</p>
<p>In terms of numbers of data sets, most researchers producing data are not producing data at large scale. The questions around how they can create good identifiers for their data, and how they can find appropriate locations to deposit their data, are equally important as for those of large data. For subject specific data there usually exists a subject specific repository. The <a href="http://www.re3data.org">Registry of Research Data Repositories</a> lists over 1500 research data repositories. The journal Scientific Data also maintains a more <a href="http://www.nature.com/sdata/policies/repositories">curated list</a>. For data that does not naturally find a home in one of these repositories there are also generic data repositories such as <a href="https://figshare.com">Figshare</a>, <a href="http://zenodo.org">Zenodo</a>, <a href="http://datadryad.org">DataDryad</a>, <a href="http://imeji.org">Imeji</a> and <a href="https://github.com">Github</a>. Each of these will allow a researcher to post a public version of their data with an appropriate identifier, usually a DOI. The main challenge here is in increasing awareness amongst researchers about these resources.</p>
<p>Data is often derived as an output of some analysis pipeline. For true reproducibility and reusability the software that was created to analyse the data also needs to be made openly available. This comes with some specific challenges around how to preserve that code, how to ensure that the code can run for other researchers, and how to manage code dependencies. The ENCODE project <span class="citation">(Hong et al. 2016)</span> tackled these problems by making virtual machines available that include all of the project data and software. Increasingly in the commercial software world treating all software and hardware dependencies as code is becoming standard practice, and this allows entire software stacks to be reproducibly built from a descriptive formula <span class="citation">(K. Morris, n.d.)</span>.</p>
<p>Privacy of data is a critical issue, however the number of domains in which this is a concern is a small subset of all researcher domains that are producing data, so I caution that discussion around privacy do need to be taken seriously, but they should not dominate the debate around policies for making data available where privacy is not a concern. The mantra “Open Where Possible” should be followed.</p>
<p>One last critical piece of infrastructure that is required for a healthy Open Data ecosystem is the ability to give credit to data. Within scholarly publishing this is done through enabling data citation. Between 2012 and 2015 a <a href="https://www.force11.org">FORCE11</a> working group created the data citation principles <span class="citation">(Altman et al. 2015)</span>. All stakeholders in the scholarly enterprise should treat data as a first class citizen, and cite it appropriately. The standard XML used in scholarly publishing has also been updated to support data citation <span class="citation">(Mietchen et al. 2015)</span>. Journals need to encourage good data citation practice, and need to ensure that data is acknowledged correctly. For example the open access journal <a href="https://elifesciences.org">eLife</a> requires author list any novel data that they create in the course of writing their paper, along with giving credit to any previously published data that they used as part of their investigation.</p>
<h2 id="recommendations-for-core-challenge-two">Recommendations for Core Challenge Two:</h2>
<ul>
<li>Advocate for smart preservation of data, taking into account the full research lifecycle.<br />
</li>
<li>Increase awareness of the many data repository options that already exist, and require deposition into an appropriate repository for any given data set.<br />
</li>
<li>Continually review data infrastructure needs, at national and trans-national levels.</li>
<li>Cite data as a first class object.</li>
</ul>
<h1 id="core-challenge-three-creating-a-supporting-culture-for-openness">Core Challenge Three: Creating a Supporting Culture for Openness</h1>
<p>This is addressed by rules five, seven, nine and ten, and can be summarised by asking how we can ensure that the correct incentives are in place to support the sharing of open data.</p>
<ul>
<li>Rule 7. State How You Want to Get Credit.<br />
</li>
<li>Rule 9. Reward Colleagues Who Share Their Data Properly.<br />
</li>
<li>Rule 10. Be a Booster for Data Science.</li>
</ul>
<p>Irrespective of how good data management skills are, or how sophisticated data hosting infrastructure is, unless there is a willingness on the part of the researcher to make their data open, then no data sharing will happen.</p>
<p>For the researcher they have to put time into making their data available, and they need to feel sure that this time would not be better used in helping their careers were they doing something else with that time, such as writing grant proposals, or working on new experiments. In some cases researchers may even have a misguided fear of sharing data in the belief that by do doing they will loose out on publishing potential results that they may have otherwise been able to obtain from the data set <span class="citation">(The International Consortium of Investigators for Fairness in Trial Data Sharing 2016)</span>.</p>
<p>To overcome these fears researchers motivations to share data need to outweigh their misgivings towards data sharing.</p>
<p>There are two strategies that can be taken towards this. The first is to make data sharing mandatory in order to receive grants or to achieve publication. The second approach is to fairly reward data and software producers for their efforts on their own merits. This is more desirable, but significantly harder to do.</p>
<p>Mandatory data sharing is most common on the journal, funder and discipline level. I am unaware of any national mandate. Since 2014 PLOS has required data to be made available for publication across all of PLOS journals. This has led to a significant increase in the number of articles public published that also make their underlying data available. The Wellcome Trust <a href="https://wellcome.ac.uk/funding/managing-grant/policy-data-management-and-sharing">requests that data be made available</a>, and the Bill and Melinda Gates Foundation <a href="http://www.gatesfoundation.org/How-We-Work/General-Information/Open-Access-Policy">require all data to be made available</a>. From a grassroots effort the disciple of crystallography evolved to one in which data sharing is now mandatory. Ad hoc practice became codified as required practice <span class="citation">(H. M. Berman et al. 2008)</span>. This seems to have been an example of where an initial tight-knit community recognised the value of data sharing, and as that community grew those values of data sharing grew with it.</p>
<p>A greater challenge is how to change implicit behaviour. In order to do this the reward system needs to be modified to ensure that researchers get appropriate credit for the creation of data and code. There is strong evidence that open publication practices reward researchers through more citations, access to better collaborations and easier adherence to funder mandates <span class="citation">(McKiernan et al. 2016)</span>. In particular papers that make their data available garner more citations than those that don’t <span class="citation">(H. A. Piwowar and Vision 2013)</span>. Nonetheless researchers remain fixated on articles, and in particular article in “high-impact” journals, as the primary means of validating their work. Ironically it is usually researchers that are assessing other researchers on grant award panels, or hiring committees. These bodies need to be given clear unambiguous instruction to reward researchers based on their full scientific contribution and the adoption of <a href="http://www.ascb.org/dora/">San Francisco Declaration on Research Assessment</a> should be mandatory for all such bodies.</p>
<p>To support a more nuanced way of assessing research continued investigation of <a href="http://altmetrics.org/manifesto/">altmetrics</a> is recommended. Tools like <a href="http://depsy.org">Depsy</a>, which indexes research software and gives information on it’s reuse, can help to highlight to impact and contribution of this software. Finding a way to do something similar for research data is critical.</p>
<p>Making data or software specific positions available within the academy can also create career opportunities for researchers whose contributions are critical, but currently undervalued by the current research assessment system.</p>
<h2 id="recommendations-for-core-challenge-three">Recommendations for Core Challenge Three:</h2>
<ul>
<li>Where possible, make data sharing mandatory for the receipt of grant funding or for publication of research articles.<br />
</li>
<li>Support and reward fields that have good data sharing practices through infrastructure support for data repositories.<br />
</li>
<li>Educate grant award committees about best practice for research assessment.<br />
</li>
<li>Create funding for explicit data and software career tracks.<br />
</li>
<li>Support ways to measure and reward data reuse.</li>
</ul>
<h1 id="further-reading">Further reading</h1>
<p>More relevant references for this topic can be found at the <a href="https://www.mendeley.com/groups/9199581/open-data-challenges/papers/">Open Data Challenges</a> group on Mendeley. The bibliography and source files for this short paper can be found on the <a href="https://github.com/IanMulvany/open-data-challenges/tree/gh-pages">Open Data Challenges</a> repository on github.</p>
<hr />
<div id="refs" class="references">
<div id="ref-galaxy2016">
<p>Afgan, Enis, Dannon Baker, Marius van den Beek, Daniel Blankenberg, Dave Bouvier, Martin Čech, John Chilton, et al. 2016. “The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update.” <em>Nucleic Acids Research</em> 44 (W1). Oxford University Press: W3–W10. doi:<a href="https://doi.org/10.1093/nar/gkw343">10.1093/nar/gkw343</a>.</p>
</div>
<div id="ref-Altman2015">
<p>Altman, Micah, Christine Borgman, Mercè Crosas, and Maryann Matone. 2015. “An introduction to the joint principles for data citation.” <em>Bulletin of the American Society for Information Science and Technology</em> 41 (3): 43–45. doi:<a href="https://doi.org/10.1002/bult.2015.1720410313">10.1002/bult.2015.1720410313</a>.</p>
</div>
<div id="ref-Benson2013">
<p>Benson, D. A., M. Cavanaugh, K. Clark, I. Karsch-Mizrachi, D. J. Lipman, J. Ostell, and E. W. Sayers. 2013. “GenBank.” <em>Nucleic Acids Research</em> 41 (D1): D36–D42. doi:<a href="https://doi.org/10.1093/nar/gks1195">10.1093/nar/gks1195</a>.</p>
</div>
<div id="ref-Berman2000">
<p>Berman, H. M. 2000. “The Protein Data Bank.” <em>Nucleic Acids Research</em> 28 (1): 235–42. doi:<a href="https://doi.org/10.1093/nar/28.1.235">10.1093/nar/28.1.235</a>.</p>
</div>
<div id="ref-berman2008">
<p>Berman, Helen M., Allen F. H., Kennard O., Motherwell W. D. S., Town W. G., Watson D. G., Arnold E., et al. 2008. “The Protein Data Bank: a historical perspective.” <em>Acta Crystallographica Section A Foundations of Crystallography</em> 64 (1). International Union of Crystallography: 88–95. doi:<a href="https://doi.org/10.1107/S0108767307035623">10.1107/S0108767307035623</a>.</p>
</div>
<div id="ref-RSOpen">
<p>Boulton, Geoffrey, Philip Campbell, Brian Collins, Peter Elias, Wendy Hall, Laurie Graeme, Onora O’Neill, et al. 2012. <em>Science as an open enterprise</em>. June. <a href="http://royalsociety.org/uploadedFiles/Royal{\_}Society{\_}Content/policy/projects/sape/2012-06-20-SAOE.pdf" class="uri">http://royalsociety.org/uploadedFiles/Royal{\_}Society{\_}Content/policy/projects/sape/2012-06-20-SAOE.pdf</a>.</p>
</div>
<div id="ref-CERN-DATA">
<p>CERN. 2009. “Data Preservation in High-Energy Physics,” no. May: 1–18.</p>
</div>
<div id="ref-Geoffrey2015">
<p>Geoffrey, Bilder, Lin Jennifer, and Neylon Cameron. 2015. “Principles for Open Scholarly Infrastructures-v1.”</p>
</div>
<div id="ref-Goodman2014">
<p>Goodman, Alyssa, Alberto Pepe, Alexander W. Blocker, Christine L. Borgman, Kyle Cranmer, Merce Crosas, Rosanne Di Stefano, et al. 2014. “Ten Simple Rules for the Care and Feeding of Scientific Data.” Edited by Philip E. Bourne. <em>PLoS Computational Biology</em> 10 (4). Public Library of Science: e1003542. doi:<a href="https://doi.org/10.1371/journal.pcbi.1003542">10.1371/journal.pcbi.1003542</a>.</p>
</div>
<div id="ref-Hong2016">
<p>Hong, Eurie L, Cricket A Sloan, Esther T Chan, Jean M Davidson, Venkat S Malladi, J Seth Strattan, Benjamin C Hitz, et al. 2016. “Principles of metadata organization at the ENCODE data coordination center.” <em>Database : The Journal of Biological Databases and Curation</em> 2016. doi:<a href="https://doi.org/10.1093/database/baw001">10.1093/database/baw001</a>.</p>
</div>
<div id="ref-KnowledgeExchange2016">
<p>Knowledge Exchange. 2016. “Funding research data management and related infrastructures,” no. May. <a href="http://repository.jisc.ac.uk/6402/1/Funding{\_}RDM{\_}{\&amp;}{\_}Related{\_}Infratsructures{\_}MAY2016{\_}v7.pdf" class="uri">http://repository.jisc.ac.uk/6402/1/Funding{\_}RDM{\_}{\&amp;}{\_}Related{\_}Infratsructures{\_}MAY2016{\_}v7.pdf</a>.</p>
</div>
<div id="ref-Komorowski2016">
<p>komorowski, Matt. 2015. “A History Of Storage Cost.” <a href="http://www.mkomo.com/cost-per-gigabyte-update" class="uri">http://www.mkomo.com/cost-per-gigabyte-update</a>.</p>
</div>
<div id="ref-Lichtman2014">
<p>Lichtman, Jeff W, Hanspeter Pfister, and Nir Shavit. 2014. “The big data challenges of connectomics.” <em>Nature Neuroscience</em> 17 (11). NIH Public Access: 1448–54. doi:<a href="https://doi.org/10.1038/nn.3837">10.1038/nn.3837</a>.</p>
</div>
<div id="ref-McKiernan2016">
<p>McKiernan, Erin C, Philip E Bourne, C Titus Brown, Stuart Buck, Amye Kenall, Jennifer Lin, Damon McDougall, et al. 2016. “How open science helps researchers succeed.” <em>ELife</em> 5. eLife Sciences Publications Limited: 372–82. doi:<a href="https://doi.org/10.7554/eLife.16800">10.7554/eLife.16800</a>.</p>
</div>
<div id="ref-mietchen2015">
<p>Mietchen, Daniel, Johanna McEntyre, Jeff Beck, Chris Maloney, and Force11 Data Citation Implementation Group. 2015. “Adapting JATS to support data citation.” National Center for Biotechnology Information (US).</p>
</div>
<div id="ref-Moedas2015">
<p>Moedas, Carlos. 2015. <em>Open Innovation, Open Science, Open to the World</em>. doi:<a href="https://doi.org/10.2777/061652">10.2777/061652</a>.</p>
</div>
<div id="ref-Morris2016">
<p>Morris, Kief. n.d. <em>Infrastructure as code : managing servers in the cloud</em>.</p>
</div>
<div id="ref-ams2016">
<p>“on Open Science.” n.d.</p>
</div>
<div id="ref-piwawar2013">
<p>Piwowar, Heather A., and Todd J. Vision. 2013. “Data reuse and the open data citation advantage.” <em>PeerJ</em> 1 (October). PeerJ Inc.: e175. doi:<a href="https://doi.org/10.7717/peerj.175">10.7717/peerj.175</a>.</p>
</div>
<div id="ref-RCKUOpen">
<p>RCUK. 2015. “Concordat On Open Research Data - Version 10,” no. July.</p>
</div>
<div id="ref-Sansone2012">
<p>Sansone, Susanna-Assunta, Philippe Rocca-Serra, Dawn Field, Eamonn Maguire, Chris Taylor, Oliver Hofmann, Hong Fang, et al. 2012. “Toward interoperable bioscience data.” <em>Nature Genetics</em> 44 (2): 121–26. doi:<a href="https://doi.org/10.1038/ng.1054">10.1038/ng.1054</a>.</p>
</div>
<div id="ref-Sharing2016">
<p>The International Consortium of Investigators for Fairness in Trial Data Sharing. 2016. “Toward Fairness in Data Sharing.” <em>New England Journal of Medicine</em> 375 (5). Massachusetts Medical Society: 405–7. doi:<a href="https://doi.org/10.1056/NEJMp1605654">10.1056/NEJMp1605654</a>.</p>
</div>
<div id="ref-Wilkinson2016">
<p>Wilkinson, Mark D., Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al. 2016. “The FAIR Guiding Principles for scientific data management and stewardship.” <em>Scientific Data</em> 3 (March). Nature Publishing Group: 160018. doi:<a href="https://doi.org/10.1038/sdata.2016.18">10.1038/sdata.2016.18</a>.</p>
</div>
<div id="ref-wilson2016">
<p>Wilson, Greg. 2016. “Software Carpentry: lessons learned.” <em>F1000Research</em> 3 (January). doi:<a href="https://doi.org/10.12688/f1000research.3-62.v2">10.12688/f1000research.3-62.v2</a>.</p>
</div>
</div>
</body>
</html>
